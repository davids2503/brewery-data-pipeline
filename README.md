# üç∫ Brewery Data Capture Pipeline

Pipeline de engenharia de dados constru√≠do para consumir, transformar e disponibilizar dados da Open Brewery DB API (https://www.openbrewerydb.org/), utilizando uma arquitetura em camadas (Medallion Architecture).

------------------------------------------------------------

## üß± Arquitetura Utilizada: Medallion (Bronze ‚Üí Silver ‚Üí Gold)

Este pipeline foi estruturado com base em boas pr√°ticas modernas de engenharia de dados, separando os dados em 3 camadas:

ü•â Bronze Layer:
- Armazena os dados brutos extra√≠dos da API.
- Formato: JSON.
- Exemplo de arquivo: bronze/breweries_raw_2025-03-22.json

ü•à Silver Layer:
- Dados curados (limpos, estruturados e otimizados).
- Particionado por estado (state).
- Formato: Parquet, com compress√£o snappy.
- Exemplo de path: silver/state=texas/breweries.parquet

ü•á Gold Layer:
- Dados agregados prontos para consumo anal√≠tico.
- M√©trica gerada: quantidade de cervejarias por tipo e estado.
- Exemplo de path: gold/breweries_summary_2025-03-22.parquet

------------------------------------------------------------

## üõ†Ô∏è Tecnologias Utilizadas

Ferramenta           | Papel
---------------------|---------------------------------------------------
Apache Airflow       | Orquestra√ß√£o das etapas da pipeline
Docker               | Isolamento e portabilidade do ambiente
AWS S3               | Armazenamento de dados em camadas
Pandas / PyArrow     | Manipula√ß√£o e formata√ß√£o dos dados
Parquet              | Formato colunar e eficiente para an√°lise
Requests             | Consumo da API p√∫blica de cervejarias

------------------------------------------------------------

## üöÄ Como Rodar o Projeto

‚úÖ Pr√©-requisitos:
- Docker e Docker Compose instalados
- Conta na AWS com acesso ao S3

Crie um bucket e configure as credenciais no arquivo .env:

AWS_ACCESS_KEY_ID=SUAS_CREDENCIAIS  
AWS_SECRET_ACCESS_KEY=SUA_SENHA  
AWS_DEFAULT_REGION=us-east-1  
S3_BUCKET_NAME=nome-do-seu-bucket  

‚ñ∂Ô∏è Passos para execu√ß√£o:

1. Subir os servi√ßos:
   docker compose up

2. Acessar o Airflow:
   Interface: http://localhost:8080  
   Usu√°rio: admin  
   Senha: admin

3. Executar o pipeline:
   - Ative o DAG: brewery_etl_pipeline  
   - Clique em ‚ÄúTrigger DAG‚Äù para iniciar a execu√ß√£o

------------------------------------------------------------

## üß™ Valida√ß√µes Implementadas

‚úÖ Verifica√ß√£o de vari√°veis de ambiente  
‚úÖ Tratamento de falhas de rede/API  
‚úÖ Reprocessamento a partir dos dados da camada Bronze  
‚úÖ Logs de sucesso e erro salvos no S3 para cada etapa
